{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "thread_id = uuid.uuid4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "openai_client = OpenAI()\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    rag_system_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": rag_system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    return call_openai(messages)\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_openai(\n",
    "    messages: List[dict], model: str = \"gpt-4o-mini\", temperature: float = 0.0\n",
    ") -> str:\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The primary purpose of LangSmith is to provide a platform for building production-grade LLM applications, focusing on observability and evaluation. It helps developers monitor and evaluate their applications to ensure reliability and confidence in deployment. By offering tools for tracing, evaluating, and testing prompts, it addresses the challenges of debugging and improving AI applications over time.\n"
     ]
    }
   ],
   "source": [
    "question = \"To start, what is the primary purpose of LangSmith, and what main problem does it solve for developers working with LLMs?\"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"thread_id\": thread_id}})\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In LangSmith, a 'trace' captures detailed information about the execution of an LLM application, including inputs, outputs, and the sequence of function calls. It allows for monitoring specific steps, such as retrieval and generation, and can include user feedback attached to any child run of the trace. This comprehensive logging aids in evaluating and optimizing the application's performance.\n"
     ]
    }
   ],
   "source": [
    "question = \"You mentioned it helps with observability. Could you elaborate on what a 'trace' in LangSmith actually captures from an LLM application's execution?\"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"thread_id\": thread_id}})\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To debug a RAG pipeline returning irrelevant information, you can filter the traces to focus on runs where user feedback indicates dissatisfaction or where specific tool calls were invoked. Analyze the filtered traces to identify patterns or errors in the retrieval or generation stages. Additionally, you can examine the metadata associated with these runs to understand the context and improve the relevance of the responses.\n"
     ]
    }
   ],
   "source": [
    "question = \"So, if a trace shows the entire lifecycle of a request, how can I use that information to specifically debug a RAG (Retrieval-Augmented Generation) pipeline that's returning irrelevant information?\"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"thread_id\": thread_id}})\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith helps evaluate and monitor application performance by providing a Usage Graph that tracks tracing spend by workspace (tenant_id), allowing you to analyze costs associated with data retention. It also offers insights into data retention settings, enabling you to optimize them for cost-effectiveness without losing historical observability. By managing retention defaults at both the organization and project levels, you can ensure efficient data handling as your application scales.\n"
     ]
    }
   ],
   "source": [
    "question = \"Besides debugging, how does LangSmith help me evaluate and monitor the performance of my application over time, especially after I've deployed it to production?\"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"thread_id\": thread_id}})\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you can use the traces you've already collected in LangSmith to automatically create datasets. You can filter interesting traces based on evaluation criteria and add them to a dataset without needing to build them from scratch. This allows for efficient dataset creation using existing data.\n"
     ]
    }
   ],
   "source": [
    "question = \"You mentioned creating datasets for evaluation. Can I use the traces I've already collected in LangSmith to automatically create these datasets, or do I need to build them from scratch?\"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"thread_id\": thread_id}})\n",
    "print(ai_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
