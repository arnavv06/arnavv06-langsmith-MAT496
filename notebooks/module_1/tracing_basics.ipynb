{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a simple chatbot. Type 'bye bye' to quit.\n",
      "\n",
      "Question: whos batman\n",
      "AI: Batman is the vigilante superhero created by DC Comics, whose alter‑ego is Bruce Wayne, a wealthy industrialist from Gotham City. He fights crime using his intellect, detective skills, and an array of gadgets, often aided by allies like Robin and Commissioner Gordon. The character has become an iconic figure in comic book and popular culture worldwide.\n",
      "\n",
      "Question: whos chacha chaudhary\n",
      "AI: I’m sorry, but I don’t have that information.\n",
      "\n",
      "Question: first president of india?\n",
      "AI: The first President of India was Dr Rajendra Prasad. He served from 1950, when India became a republic, until 1962. His tenure laid the groundwork for the country's early constitutional governance.\n",
      "\n",
      "Question: whos chacha chaudhary\n",
      "AI: Chacha Chaudhary is a popular Indian comic‑book hero created by Pran Kumar Sharma. He’s a clever, super‑intelligent old man who solves problems with a single “brain” punch. The character first appeared in 1971 and has since become a beloved icon in India and abroad.\n",
      "\n",
      "Question: whats the date today\n",
      "AI: Today is October 4, 2025.\n",
      "\n",
      "Question: bye bye\n",
      "au revoir\n"
     ]
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "from utils import get_vector_db_retriever\n",
    "from groq import Groq\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "\n",
    "MODEL_PROVIDER = \"groq\"\n",
    "MODEL_NAME = \"openai/gpt-oss-20b\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "groq_client = Groq()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "@traceable\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    return call_groq(messages)\n",
    "\n",
    "\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "\n",
    "@traceable\n",
    "def retrieve_documents(question: str):\n",
    "    \"\"\"Fetches documents from the vector store based on the question.\"\"\"\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable(\n",
    "    metadata={\"model_name\": MODEL_NAME, \"model_provider\": MODEL_PROVIDER}\n",
    ")\n",
    "def call_groq(\n",
    "    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n",
    ") -> str:\n",
    "    return groq_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "\n",
    "@traceable\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "@traceable\n",
    "def chat_bot():\n",
    "    print(\"This is a simple chatbot. Type 'bye bye' to quit.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        ques = input(\"What's your question: \")\n",
    "        print(f\"Question: {ques}\")\n",
    "        \n",
    "        if ques.lower() == 'bye bye':\n",
    "            print(\"au revoir\")\n",
    "            break\n",
    "        \n",
    "        response = langsmith_rag(ques)\n",
    "        print(f\"AI: {response}\\n\")\n",
    "\n",
    "\n",
    "chat_bot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "d:\\github_projects\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "from langchain_groq import ChatGroq  # Groq LLM wrapper\n",
    "\n",
    "MODEL_PROVIDER = \"groq\"\n",
    "MODEL_NAME = \"llama-3.1-8b-instant\"   # free fast Groq model\n",
    "APP_VERSION = 1.0\n",
    "\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "# Groq client\n",
    "groq_client = ChatGroq(model=MODEL_NAME, temperature=0.0)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "\"\"\"\n",
    "retrieve_documents\n",
    "- Returns documents fetched from a vectorstore based on the user's question\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "\"\"\"\n",
    "generate_response\n",
    "- Calls `call_groq` to generate a model response after formatting inputs\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": RAG_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"}\n",
    "    ]\n",
    "    return call_groq(messages)\n",
    "\n",
    "\"\"\"\n",
    "call_groq\n",
    "- Returns the chat completion output from Groq\n",
    "\"\"\"\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_groq(\n",
    "    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n",
    ") -> str:\n",
    "    # ChatGroq uses LCEL invoke, not openai-style\n",
    "    response = groq_client.invoke(messages)\n",
    "    return response.content  # returns plain string\n",
    "\n",
    "\"\"\"\n",
    "langsmith_rag\n",
    "- Calls `retrieve_documents` to fetch documents\n",
    "- Calls `generate_response` to generate a response based on the fetched documents\n",
    "- Returns the model response\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How does LangSmith help developers with their applications?\"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"website\": \"www.google.com\"}})\n",
    "print(ai_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
